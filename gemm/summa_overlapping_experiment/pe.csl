// Copyright 2025 Cerebras Systems.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// This program implements the SUMMA matrix multiplication algorithm and is
// written as an example to show how to use the `collectives_2d` library.

// We perform GEMM in `P` many steps on a grid of `P x P` processors.
// At each step `i`, PEs in the `i`th column broadcast their home tiles of `A`
// to other PEs in their row, and PEs in the `i`th row broadcast their home
// tiles of `B` to other PEs in their column. Once both broadcasts are complete
// as determined by `x_done()` and `y_done()` both being activated,
// each PE computes `C_tile += Ap * Bp` where `Ap` and `Bp` are pointers to
// either the PE's home tile or the tile it received through broadcasts.

param c2d_params: comptime_struct;
param memcpy_params: comptime_struct;

// Matrix size params
param Mt: i16;
param Kt: i16;
param Nt: i16;

// Task IDs
const EXIT:            local_task_id = @get_local_task_id(12);
const compute_task_id: local_task_id = @get_local_task_id(13);
const x_task_id:       local_task_id = @get_local_task_id(14);
const y_task_id:       local_task_id = @get_local_task_id(15);

const mpi_x = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2d_params.x,
    .queues = [2]u16{2,4},
    .dest_dsr_ids = [1]u16{1},
    .src0_dsr_ids = [1]u16{1},
    .src1_dsr_ids = [1]u16{1}
    });
const mpi_y = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2d_params.y,
    .queues = [2]u16{3,5},
    .dest_dsr_ids = [1]u16{2},
    .src0_dsr_ids = [1]u16{2},
    .src1_dsr_ids = [1]u16{2}
    });

// On WSE-2, memcpy uses input/output queue 0
// On WSE-3, memcpy uses input/output queues 0 and 1
const sys_mod = @import_module("<memcpy/memcpy>", memcpy_params);

const P = @get_rectangle().width;

// This PE's home tile of A, B, C
// `A_tile` and `B_tile` will be populated with initial values by run.py
// These arrays are stored in a column major format.
var A_tile = @zeros([Mt*Kt]f32);
var B_tile = @zeros([Kt*Nt]f32);
var C_tile = @zeros([Mt*Nt]f32);

var ptr_A : [*]f32 = &A_tile;
var ptr_B : [*]f32 = &B_tile;
var ptr_C : [*]f32 = &C_tile;

// Double buffers for overlapping communication and computation
// Buffer 0 is used for even steps, buffer 1 for odd steps
var A_buffer_0 = @zeros([Mt*Kt]f32);
var A_buffer_1 = @zeros([Mt*Kt]f32);
var B_buffer_0 = @zeros([Kt*Nt]f32);
var B_buffer_1 = @zeros([Kt*Nt]f32);

// Helper functions to get the correct buffer based on step
fn get_A_buffer(s: u16) *[Mt*Kt]f32 {
    return if (s % 2 == 0) &A_buffer_0 else &A_buffer_1;
}

fn get_B_buffer(s: u16) *[Kt*Nt]f32 {
    return if (s % 2 == 0) &B_buffer_0 else &B_buffer_1;
}

var px: u16;
var py: u16;

task x_done() void {
  @activate(compute_task_id);
}

task y_done() void {
  @unblock(compute_task_id);
}

var step: u16 = 0;

// main() is called once by the host to start the algorithm
// It initializes state and starts the first broadcast (step 0)
fn main() void {
  // Initialize MPI state
  mpi_x.init();
  mpi_y.init();
  px = mpi_x.pe_id;
  py = mpi_y.pe_id;

  // Start broadcast for step 0
  // Use buffer[0] for step 0
  const Ap = if (px == 0) &A_tile else get_A_buffer(0);
  const Bp = if (py == 0) &B_tile else get_B_buffer(0);
  mpi_x.broadcast(0, @ptrcast([*]u32, Ap), Mt * Kt, x_task_id);
  mpi_y.broadcast(0, @ptrcast([*]u32, Bp), Kt * Nt, y_task_id);
}

task compute() void {
  const Ap = if (px == step) &A_tile else get_A_buffer(step);
  const Bp = if (py == step) &B_tile else get_B_buffer(step);


  if (step + 1 < P) {
    const next_step: u16 = step + 1;
    const next_Ap = if (px == next_step) &A_tile else get_A_buffer(next_step);
    const next_Bp = if (py == next_step) &B_tile else get_B_buffer(next_step);
    mpi_x.broadcast(next_step, @ptrcast([*]u32, next_Ap), Mt * Kt, x_task_id);
    mpi_y.broadcast(next_step, @ptrcast([*]u32, next_Bp), Kt * Nt, y_task_id);
  }

  // GEMM
  var A_dsd  = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> A_tile[i] });
  A_dsd = @set_dsd_base_addr(A_dsd, Ap);

  for (@range(i16, Kt)) |k| {
    var C_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> C_tile[i] });

    for (@range(i16, Nt)) |j| {
      const b = Bp.*[j*Kt + k];
      @fmacs(C_dsd, C_dsd, A_dsd, b);
      C_dsd = @increment_dsd_offset(C_dsd, Mt, f32);
    }
    A_dsd = @increment_dsd_offset(A_dsd, Mt, f32);
  }

  step += 1;
  @block(compute_task_id);

  // Next compute will be triggered by x_done/y_done from broadcast
  if (step == P) {
    @activate(EXIT);
  }
}

task f_exit() void {
  // the user must unblock cmd color for every PE
  sys_mod.unblock_cmd_stream();
}

comptime {
  @bind_local_task(f_exit, EXIT);
  @bind_local_task(compute, compute_task_id);
  @bind_local_task(x_done, x_task_id);
  @bind_local_task(y_done, y_task_id);
  @block(compute_task_id);
  @export_symbol(ptr_C, "C");

  @export_symbol(ptr_A, "A");
  @export_symbol(ptr_B, "B");
  @export_symbol(main);
}
