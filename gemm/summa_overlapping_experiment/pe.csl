// Copyright 2025 Cerebras Systems.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// This program implements the SUMMA matrix multiplication algorithm and is
// written as an example to show how to use the `collectives_2d` library.

// We perform GEMM in `P` many steps on a grid of `P x P` processors.
// At each step `i`, PEs in the `i`th column broadcast their home tiles of `A`
// to other PEs in their row, and PEs in the `i`th row broadcast their home
// tiles of `B` to other PEs in their column. Once both broadcasts are complete
// as determined by `x_done()` and `y_done()` both being activated,
// each PE computes `C_tile += Ap * Bp` where `Ap` and `Bp` are pointers to
// either the PE's home tile or the tile it received through broadcasts.

param c2d_params: comptime_struct;
param memcpy_params: comptime_struct;

// Matrix size params
param Mt: i16;
param Kt: i16;
param Nt: i16;

// Task IDs
const EXIT:            local_task_id = @get_local_task_id(12);
const compute_task_id: local_task_id = @get_local_task_id(13);
const x_task_id:       local_task_id = @get_local_task_id(14);
const y_task_id:       local_task_id = @get_local_task_id(15);

const mpi_x = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2d_params.x,
    .queues = [2]u16{2,4},
    .dest_dsr_ids = [1]u16{1},
    .src0_dsr_ids = [1]u16{1},
    .src1_dsr_ids = [1]u16{1}
    });
const mpi_y = @import_module("<collectives_2d/pe>", .{
    .dim_params = c2d_params.y,
    .queues = [2]u16{3,5},
    .dest_dsr_ids = [1]u16{2},
    .src0_dsr_ids = [1]u16{2},
    .src1_dsr_ids = [1]u16{2}
    });

// On WSE-2, memcpy uses input/output queue 0
// On WSE-3, memcpy uses input/output queues 0 and 1
const sys_mod = @import_module("<memcpy/memcpy>", memcpy_params);

const P = @get_rectangle().width;

// This PE's home tile of A, B, C
// `A_tile` and `B_tile` will be populated with initial values by run.py
// These arrays are stored in a column major format.
var A_tile = @zeros([Mt*Kt]f32);
var B_tile = @zeros([Kt*Nt]f32);
var C_tile = @zeros([Mt*Nt]f32);

var ptr_A : [*]f32 = &A_tile;
var ptr_B : [*]f32 = &B_tile;
var ptr_C : [*]f32 = &C_tile;

// Double buffers for pipelining
// Allows broadcasts to be 1 step ahead of compute
var A_buffer_0 = @zeros([Mt*Kt]f32);
var A_buffer_1 = @zeros([Mt*Kt]f32);
var B_buffer_0 = @zeros([Kt*Nt]f32);
var B_buffer_1 = @zeros([Kt*Nt]f32);

// Track which compute step we're on
var compute_step: u16 = 0;

// Track which broadcast to launch next
var next_broadcast_step: u16 = 1;  // Start at 1 (step 0 launched by main)

var px: u16;
var py: u16;

task x_done() void {
  @activate(compute_task_id);  // Wake compute for completed broadcast

  // *** Immediately initiate next X-direction broadcast ***
  // But only if we're not too far ahead of compute (at most 1 step ahead)
  if (next_broadcast_step < P and next_broadcast_step < compute_step + 2) {
    const step = next_broadcast_step;
    const buf_idx = step % 2;
    const Ap = if (px == step) &A_tile
               else if (buf_idx == 0) &A_buffer_0
               else &A_buffer_1;

    mpi_x.broadcast(step, @ptrcast([*]u32, Ap), Mt * Kt, x_task_id);
    // Don't increment here - y_done needs the same step!
  }
}

task y_done() void {
  @unblock(compute_task_id);   // Allow compute to run

  // *** Immediately initiate next Y-direction broadcast ***
  // But only if we're not too far ahead of compute (at most 1 step ahead)
  if (next_broadcast_step < P and next_broadcast_step < compute_step + 2) {
    const step = next_broadcast_step;
    const buf_idx = step % 2;
    const Bp = if (py == step) &B_tile
               else if (buf_idx == 0) &B_buffer_0
               else &B_buffer_1;

    mpi_y.broadcast(step, @ptrcast([*]u32, Bp), Kt * Nt, y_task_id);
    // Increment AFTER both x and y broadcasts are launched
    next_broadcast_step += 1;
  }
}

fn main() void {
  // Initialize MPI and initiate first broadcast directly
  mpi_x.init();
  mpi_y.init();
  px = mpi_x.pe_id;
  py = mpi_y.pe_id;

  // Directly initiate broadcast for step 0
  const Ap = if (px == 0) &A_tile else &A_buffer_0;
  const Bp = if (py == 0) &B_tile else &B_buffer_0;

  mpi_x.broadcast(0, @ptrcast([*]u32, Ap), Mt * Kt, x_task_id);
  mpi_y.broadcast(0, @ptrcast([*]u32, Bp), Kt * Nt, y_task_id);
}

task compute() void {
  // Select buffer based on compute_step (matches the broadcast that filled it)
  const buf_idx = compute_step % 2;
  const Ap = if (px == compute_step) &A_tile
             else if (buf_idx == 0) &A_buffer_0
             else &A_buffer_1;
  const Bp = if (py == compute_step) &B_tile
             else if (buf_idx == 0) &B_buffer_0
             else &B_buffer_1;

  // Perform local GEMM for CURRENT step
  // Next broadcast is launched by x_done/y_done when previous broadcast completes
  var A_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> A_tile[i] });
  A_dsd = @set_dsd_base_addr(A_dsd, Ap);

  for (@range(i16, Kt)) |k| {
    var C_dsd = @get_dsd(mem1d_dsd, .{ .tensor_access = |i|{Mt} -> C_tile[i] });

    for (@range(i16, Nt)) |j| {
      const b = Bp.*[j*Kt + k];
      @fmacs(C_dsd, C_dsd, A_dsd, b);
      C_dsd = @increment_dsd_offset(C_dsd, Mt, f32);
    }
    A_dsd = @increment_dsd_offset(A_dsd, Mt, f32);
  }

  compute_step += 1;
  @block(compute_task_id);

  if (compute_step == P) {
    @activate(EXIT);
  }
}

task f_exit() void {
  // the user must unblock cmd color for every PE
  sys_mod.unblock_cmd_stream();
}

comptime {
  @bind_local_task(f_exit, EXIT);
  @bind_local_task(compute, compute_task_id);
  @bind_local_task(x_done, x_task_id);
  @bind_local_task(y_done, y_task_id);

  // Compute task starts blocked
  @block(compute_task_id);

  @export_symbol(ptr_C, "C");
  @export_symbol(ptr_A, "A");
  @export_symbol(ptr_B, "B");
  @export_symbol(main);
}
